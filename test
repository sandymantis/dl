import torch

if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("MPS is available! Using MPS device.")
else:
    device = torch.device("cpu")
    print("MPS is not available. Using CPU.")



Hereâ€™s a summary of the key points:
1. Current Scope of Gaps in Care API
â€¢ The API is read-only and provides providers with a list of open gaps for the current calendar year (p only, not re).
â€¢ The data flows from a/b â†’ Snowflake (historical storage) â†’ Cosmos DB (API data).
â€¢ Only current-year gaps are stored in Cosmos DB to ensure providers focus on what they are incentivized to close.
2. Challenges & Enhancements Needed
a. Soft Closure Tracking
â€¢ Today, there is no way to record if a provider is actively working on closing a gap (soft closure).
â€¢ This creates an issue where multiple providers may unknowingly work on the same gap, leading to inefficiencies.
â€¢ A real-time mechanism to capture ongoing work needs to be implemented.
b. Data Load Strategy for Cosmos DB
â€¢ Need to ensure up-to-date open gaps data in Cosmos DB, regardless of whether using an incremental or full load strategy.
â€¢ Review audit mechanisms to confirm completeness and accuracy.
c. Expanding API Scope to Support Non-U Populations
â€¢ Currently supports only U patients; needs to be extended to non-U populations.
â€¢ Needs to handle gaps identified through home and community programs for both U and non-U 
â€¢ Transform into a packaged API to support broader use cases.

model = model.to(device)  # Move model to MPS (GPU)
input_tensor = input_tensor.to(device)  # Move input data to MPS



import torchvision
import torch
from PIL import Image

# Create a vector of zeros of size 5
size = (128, 128)

# Define transformations for resizing and converting to tensor
transform = torchvision.transforms.Compose([
    torchvision.transforms.Resize(size),
    torchvision.transforms.ToTensor()
])

# Load the Flowers102 dataset for training and testing
train_dataset = list(torchvision.datasets.Flowers102("./flowers", "train", transform=transform, download=True))
test_dataset = list(torchvision.datasets.Flowers102("./flowers", "test", transform=transform, download=True))

# Define a function to visualize an image
def visualize_image(img: torch.Tensor) -> Image.Image:
    return Image.fromarray((img.permute(1, 2, 0) * 255).to(torch.uint8).numpy())

# Visualize the first image in the train dataset
visualize_image(train_dataset[1][0])



train_images = torch.stack([im for im, _ in train_dataset], dim=0)  # Stack all images in the train dataset
train_label = torch.tensor([label for _, label in train_dataset])  # Extract labels from the train dataset


import matplotlib.pyplot as plt

f, ax = plt.subplots(4, 10, figsize=(10, 5))

for i, (im, l) in enumerate(list(train_dataset)[:40]):
    ax[i // 10, i % 10].imshow(visualize_image(im))
    ax[i // 10, i % 10].set_title(l)
    ax[i // 10, i % 10].axis('off')


model = torch.nn.Linear(128 * 128 * 3, 1)  # Define a linear model
loss = torch.nn.MSELoss()  # Define the loss function
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # Define the optimizer

for epoch in range(10):  # Training loop for 10 epochs
    pred_label = model(train_images.view(-1, 128 * 128 * 3))  # Forward pass
    print(pred_label.shape, train_label.shape)  # Print the shape of the prediction and target

    loss_val = loss(pred_label.view(-1), train_label.float())  # Compute the loss

    optimizer.zero_grad()  # Zero the gradients
    loss_val.backward()  # Backpropagation
    optimizer.step()  # Update the weights

    print(f"Epoch {epoch}, loss {loss_val.item()}")  # Print loss for the epoch


