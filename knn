import torch
import torchvision
from torchvision.transforms import Compose, Resize, ToTensor
from PIL import Image
import matplotlib.pyplot as plt

# **Step 1: Dataset Preparation**
# Define the image size and transformations
size = (128, 128)
transform = Compose([
    Resize(size),
    ToTensor()
])


Root Cause:

I’d like to take a moment to share some reflections on our talent strategy. A few recent decisions has sparked conversations around fairness, transparency, and meritocracy. While this perspective may differ from the prevailing view, I encourage you to consider it with an open mind, as I truly believe that with time and reflection, the broader issue will become clearer.

I genuinely appreciate the time and effort our leadership team invests in identifying the right talent. I have no doubt that the intentions behind these decisions are well-meaning and that significant thought goes into the process. Hence, it’s worth asking—where might we be falling short? A part of me wishes that talent decisions were as straightforward as a game of tennis, where you step onto the court, play, and the winner is clear for all to see. Unfortunately, talent decisions aren’t as objective, which is where the challenge arises.
Right now, many of our decisions seem to be shaped by perception and second-hand information rather than direct, firsthand evaluation. This naturally creates an environment where individuals or their delegates feel the need to manage perceptions rather than focus purely on impact. Over time, this can lead to a culture of inauthenticity—not because anyone intends for it to happen, but as an unintended consequence of the way our system is structured. While this isn’t a reflection of any single decision or person, it’s a systemic challenge that deserves attention.
So, how do we address this in a way that strengthens fairness, transparency, and objectivity? One approach I would like to propose is implementing a panel evaluation process for key talent decisions—especially promotions for levels 31+, where the number of eligible candidates per year is relatively small.
In this setup, candidates would have the opportunity to present their body of work directly to a qualified, independent panel that includes members of the leadership team. This would allow decision-makers to assess contributions firsthand rather than relying on second-hand information or perception. At the same time, it gives candidates a structured platform to articulate their impact, receive actionable feedback, and understand what they can do to continue growing.
Given the size and scale of our team, it’s understandable that not everyone in the leadership team will have deep familiarity with every candidate’s work. A panel-based approach would help bridge that gap by providing a more objective, consistent, and transparent evaluation process.
This is not about questioning past decisions but about enhancing the process for the future. The goal is to create a process that is not only fair and transparent but also provides a level playing field where every individual has an equal opportunity to be assessed on their true impact.
I appreciate your time and consideration and look forward to any thoughts you may have on this.

PS: Seeing it to all who I am told are part of the talent disciossn + k 

//
Thanks for your time yesterday. I wanted to summarize the key point of why I reached out.
Over the past few years, whenever we discussed talent strategy, you always took the names of A and B in the same breath. My understanding was that you were advocating for both of us—it just hadn’t been approved due to organizational constraints. However, given how things unfolded this year, there seems to be a shift from our past discussions, which is why I had these questions.
I know you were short on time yesterday, so I would like to continue this conversation when you're back. In the meantime, I will work with Kristy on the proposed operating model for arch.
Safe travels!


from torchvision import datasets
from torchvision.transforms import ToTensor

train_data = datasets.FashionMNIST(root="data", train=True, download=True, transform=ToTensor())
test_data = datasets.FashionMNIST(root="data", train=False, download=True, transform=ToTensor())

# Extract train and test labels
train_data_labels = train_data.targets
test_data_labels = test_data.targets


import matplotlib.pyplot as plt
figure = plt.figure(figsize=(8, 8))
figure.add_subplot(1,6,1)

i=1000
img, label = train_data[i]
# print("img squeeze ", img.squeeze().shape)
# print("img without squeeze ", img.shape)

plt.title(label)
# plt.axis("off")
plt.imshow(img.squeeze(), cmap="gray")

# figure.add_subplot(1,6,6)
# img, label = train_data[200]

# plt.title(label)
# plt.imshow(img.squeeze(), cmap="gray")


# plt.show()


# Model definition #

model = torch.nn.Linear(28*28,10)
# model.parameters() #returns the weights and bias
loss = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), 0.1)

# Training Step #

# print("Weights = ", model.weight.shape)
# print("Bias = ", model.bias)

wt_mean_square = model.weight.pow(2).mean()
print("Weights MSE before training", wt_mean_square.item())

bias_mean_square = model.bias.pow(2).mean()
print("Bias MSE before training", bias_mean_square.item())

metrics = {"train_acc": [], "val_acc": []}

for i in range(60000):
    pred = model(train_data[i][0].squeeze().flatten())
    # print("Pred = ", pred)
    # print("Label = ", train_data[i][1])
    # print("Step ", i)

    optimizer.zero_grad()
    loss_val = loss(pred,torch.tensor(train_data[i][1]))
    loss_val.backward()
    # print("Weights Grad = ", model.weight.grad.pow(2).mean().item())
    optimizer.step()
    train_acc = (torch.argmax(pred) == train_data[i][1]).sum()
    print("Pred = ", torch.argmax(pred), " Label = ", train_data[i][1], " Match = ", torch.argmax(pred) == train_data[i][1])
    metrics["train_acc"].append(train_acc.item())


# print("Weights = ", model.weight)
# print("Bias = ", model.bias)
print("metrics[train_acc] = ", metrics["train_acc"])
print("Accuracy % = ", sum(metrics["train_acc"]) / len(metrics["train_acc"]) * 100)


wt_mean_square = model.weight.pow(2).mean()
print("Weights MSE after training", wt_mean_square.item())

bias_mean_square = model.bias.pow(2).mean()
print("Bias MSE after training", bias_mean_square.item())



# Validation #
metrics = {"val_acc": [], "preds": []}

labels_map = {
    0: "T-Shirt",
    1: "Trouser",
    2: "Pullover",
    3: "Dress",
    4: "Coat",
    5: "Sandal",
    6: "Shirt",
    7: "Sneaker",
    8: "Bag",
    9: "Ankle Boot",
}

for i in range(10000):
    pred = model(train_data[i][0].squeeze().flatten())
    # print(torch.argmax(pred)==train_data[i][1])
    metrics["preds"].append(labels_map[torch.argmax(pred).item()])
    acc = (torch.argmax(pred)==train_data[i][1]).sum()
    metrics["val_acc"].append(acc.item())

print("Accuracy % =", sum(metrics["val_acc"])/ len(metrics["val_acc"]) * 100)
print("Pred Accuracy =",metrics["val_acc"] )
print("Predictions =",metrics["preds"] )

import matplotlib.pyplot as plt
figure = plt.figure(figsize=(8, 8))
figure.add_subplot(1,6,1)

i=2
img, label = test_data[i]
# print("img squeeze ", img.squeeze().shape)
# print("img without squeeze ", img.shape)

plt.title(label)
# plt.axis("off")
plt.imshow(img.squeeze(), cmap="gray")

//
# Load the Flowers102 dataset
train_dataset = torchvision.datasets.Flowers102(
    "./flowers", "train", transform=transform, download=True
)
test_dataset = torchvision.datasets.Flowers102(
    "./flowers", "train", transform=transform, download=True
)

import torch
import torch.nn as nn

class MyModel(torch.nn.Module):
    def __init__(self, layer_size=[512, 512, 512]) -> None:
        super().__init__()
        layers = []
        layers.append(torch.nn.Flatten())
        c = 128 * 128 * 3
        for s in layer_size:
            layers.append(torch.nn.Linear(c, s))
            layers.append(torch.nn.ReLU())
            c = s
        layers.append(torch.nn.Linear(c, 102))
        self.model = torch.nn.Sequential(*layers)

    def forward(self, x) -> Any:
        return self.model(x)


# **Step 2: Visualizing Images**
def visualize_image(img: torch.Tensor) -> Image.Image:
    return Image.fromarray(
        (img.permute(1, 2, 0) * 255).to(torch.uint8).numpy()
    )

# Visualize the first 40 images
f, ax = plt.subplots(4, 10, figsize=(12, 6))
for i, (im, l) in enumerate(list(train_dataset)[:40]):
    ax[i // 10, i % 10].imshow(visualize_image(im))
    ax[i // 10, i % 10].set_title(l)
    ax[i // 10, i % 10].axis("off")
plt.tight_layout()
plt.show()

# **Step 3: Prepare Classes**
class_0 = list(train_dataset)[:10]
class_1 = list(train_dataset)[10:20]
class_01 = class_0 + class_1  # Combined class

# **Step 4: KNN Classifier**
def knn_classifier(x, k=3):
    dist = [((x - im).pow(2).sum(), l) for im, l in class_01]
    k_closest = [l for _, l in sorted(dist)[:k]]
    return sorted(k_closest)[k // 2]  # Return the median of the k closest labels

# **Step 5: Accuracy Calculation for Classifier**
accuracy = sum(knn_classifier(x) == l for x, l in list(test_dataset)[:20]) / 20
print(f"KNN Classifier Accuracy: {accuracy:.2f}")

# **Step 6: KNN Regression**
def knn_regression(x, k=3):
    dist = [((x - im).pow(2).sum(), l) for im, l in class_01]
    k_closest = [l for _, l in sorted(dist)[:k]]
    return torch.mean(torch.tensor(k_closest).float())

# **Step 7: Example for KNN Regression**
predicted_value = knn_regression(test_dataset[3][0])
print(f"KNN Regression Predicted Value: {predicted_value.item()}")
