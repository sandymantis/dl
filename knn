import torch
import torchvision
from torchvision.transforms import Compose, Resize, ToTensor
from PIL import Image
import matplotlib.pyplot as plt

# **Step 1: Dataset Preparation**
# Define the image size and transformations
size = (128, 128)
transform = Compose([
    Resize(size),
    ToTensor()
])

import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt

# Load an image from file
image_path = "your_image.jpg"  # Replace with your image path
image = Image.open(image_path).convert("RGB")  # Ensure it's RGB (3 channels)

# Define transformations
transform = transforms.Compose([
    transforms.Resize((64, 64)),  # Resize to 64x64
    transforms.ToTensor(),  # Convert to tensor (C, H, W)
])

# Apply transformations
x = transform(image)  # Shape: (3, 64, 64)

# Add batch dimension to match (1, 3, 64, 64)
x = x.unsqueeze(0)  # Shape: (1, 3, 64, 64)

# Display the loaded image
plt.imshow(image)
plt.title("Loaded Image")
plt.axis("off")
plt.show()


import torch
import torch.nn as nn

class ConvNet(nn.Module):
    class Block(nn.Module):
        def __init__(self, in_channels, out_channels, kernel_size, stride):
            super().__init__()
            padding = kernel_size // 2
            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
            self.relu = nn.ReLU()

        def forward(self, x):
            return self.relu(self.conv(x))

    def __init__(self, channels_1_0=64, n_blocks=4):
        super().__init__()

        cnn_layers = [
            nn.Conv2d(3, channels_1_0, kernel_size=11, stride=2, padding=5),
            nn.ReLU(),
        ]

        layers = [64, 128, 256, 512]  # Example channel sizes for the blocks
        c1 = channels_1_0

        for c2 in layers:
            cnn_layers.append(self.Block(c1, c2, kernel_size=3, stride=1))
            c1 = c2  # Current output channels become next input channels

        cnn_layers.append(nn.Conv2d(c1, 1, kernel_size=1))
        # Optional: Apply global average pooling to reduce spatial size to 1x1
        # cnn_layers.append(nn.AdaptiveAvgPool2d(1))

        self.network = nn.Sequential(*cnn_layers)

    def forward(self, x):
        return self.network(x)





from torchvision import datasets
from torchvision.transforms import ToTensor

train_data = datasets.FashionMNIST(root="data", train=True, download=True, transform=ToTensor())
test_data = datasets.FashionMNIST(root="data", train=False, download=True, transform=ToTensor())

# Extract train and test labels
train_data_labels = train_data.targets
test_data_labels = test_data.targets


import matplotlib.pyplot as plt
figure = plt.figure(figsize=(8, 8))
figure.add_subplot(1,6,1)

i=1000
img, label = train_data[i]
# print("img squeeze ", img.squeeze().shape)
# print("img without squeeze ", img.shape)

plt.title(label)
# plt.axis("off")
plt.imshow(img.squeeze(), cmap="gray")

# figure.add_subplot(1,6,6)
# img, label = train_data[200]

# plt.title(label)
# plt.imshow(img.squeeze(), cmap="gray")


# plt.show()


# Model definition #

model = torch.nn.Linear(28*28,10)
# model.parameters() #returns the weights and bias
loss = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), 0.1)

# Training Step #

# print("Weights = ", model.weight.shape)
# print("Bias = ", model.bias)

wt_mean_square = model.weight.pow(2).mean()
print("Weights MSE before training", wt_mean_square.item())

bias_mean_square = model.bias.pow(2).mean()
print("Bias MSE before training", bias_mean_square.item())

metrics = {"train_acc": [], "val_acc": []}

for i in range(60000):
    pred = model(train_data[i][0].squeeze().flatten())
    # print("Pred = ", pred)
    # print("Label = ", train_data[i][1])
    # print("Step ", i)

    optimizer.zero_grad()
    loss_val = loss(pred,torch.tensor(train_data[i][1]))
    loss_val.backward()
    # print("Weights Grad = ", model.weight.grad.pow(2).mean().item())
    optimizer.step()
    train_acc = (torch.argmax(pred) == train_data[i][1]).sum()
    print("Pred = ", torch.argmax(pred), " Label = ", train_data[i][1], " Match = ", torch.argmax(pred) == train_data[i][1])
    metrics["train_acc"].append(train_acc.item())


# print("Weights = ", model.weight)
# print("Bias = ", model.bias)
print("metrics[train_acc] = ", metrics["train_acc"])
print("Accuracy % = ", sum(metrics["train_acc"]) / len(metrics["train_acc"]) * 100)


wt_mean_square = model.weight.pow(2).mean()
print("Weights MSE after training", wt_mean_square.item())

bias_mean_square = model.bias.pow(2).mean()
print("Bias MSE after training", bias_mean_square.item())



# Validation #
metrics = {"val_acc": [], "preds": []}

labels_map = {
    0: "T-Shirt",
    1: "Trouser",
    2: "Pullover",
    3: "Dress",
    4: "Coat",
    5: "Sandal",
    6: "Shirt",
    7: "Sneaker",
    8: "Bag",
    9: "Ankle Boot",
}

for i in range(10000):
    pred = model(train_data[i][0].squeeze().flatten())
    # print(torch.argmax(pred)==train_data[i][1])
    metrics["preds"].append(labels_map[torch.argmax(pred).item()])
    acc = (torch.argmax(pred)==train_data[i][1]).sum()
    metrics["val_acc"].append(acc.item())

print("Accuracy % =", sum(metrics["val_acc"])/ len(metrics["val_acc"]) * 100)
print("Pred Accuracy =",metrics["val_acc"] )
print("Predictions =",metrics["preds"] )

import matplotlib.pyplot as plt
figure = plt.figure(figsize=(8, 8))
figure.add_subplot(1,6,1)

i=2
img, label = test_data[i]
# print("img squeeze ", img.squeeze().shape)
# print("img without squeeze ", img.shape)

plt.title(label)
# plt.axis("off")
plt.imshow(img.squeeze(), cmap="gray")

//
# Load the Flowers102 dataset
train_dataset = torchvision.datasets.Flowers102(
    "./flowers", "train", transform=transform, download=True
)
test_dataset = torchvision.datasets.Flowers102(
    "./flowers", "train", transform=transform, download=True
)

import torch
import torch.nn as nn

class MyModel(torch.nn.Module):
    def __init__(self, layer_size=[512, 512, 512]) -> None:
        super().__init__()
        layers = []
        layers.append(torch.nn.Flatten())
        c = 128 * 128 * 3
        for s in layer_size:
            layers.append(torch.nn.Linear(c, s))
            layers.append(torch.nn.ReLU())
            c = s
        layers.append(torch.nn.Linear(c, 102))
        self.model = torch.nn.Sequential(*layers)

    def forward(self, x) -> Any:
        return self.model(x)


# **Step 2: Visualizing Images**
def visualize_image(img: torch.Tensor) -> Image.Image:
    return Image.fromarray(
        (img.permute(1, 2, 0) * 255).to(torch.uint8).numpy()
    )

# Visualize the first 40 images
f, ax = plt.subplots(4, 10, figsize=(12, 6))
for i, (im, l) in enumerate(list(train_dataset)[:40]):
    ax[i // 10, i % 10].imshow(visualize_image(im))
    ax[i // 10, i % 10].set_title(l)
    ax[i // 10, i % 10].axis("off")
plt.tight_layout()
plt.show()

# **Step 3: Prepare Classes**
class_0 = list(train_dataset)[:10]
class_1 = list(train_dataset)[10:20]
class_01 = class_0 + class_1  # Combined class

# **Step 4: KNN Classifier**
def knn_classifier(x, k=3):
    dist = [((x - im).pow(2).sum(), l) for im, l in class_01]
    k_closest = [l for _, l in sorted(dist)[:k]]
    return sorted(k_closest)[k // 2]  # Return the median of the k closest labels

# **Step 5: Accuracy Calculation for Classifier**
accuracy = sum(knn_classifier(x) == l for x, l in list(test_dataset)[:20]) / 20
print(f"KNN Classifier Accuracy: {accuracy:.2f}")

# **Step 6: KNN Regression**
def knn_regression(x, k=3):
    dist = [((x - im).pow(2).sum(), l) for im, l in class_01]
    k_closest = [l for _, l in sorted(dist)[:k]]
    return torch.mean(torch.tensor(k_closest).float())

# **Step 7: Example for KNN Regression**
predicted_value = knn_regression(test_dataset[3][0])
print(f"KNN Regression Predicted Value: {predicted_value.item()}")
